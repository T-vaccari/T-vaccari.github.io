<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4001/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4001/" rel="alternate" type="text/html" /><updated>2025-03-14T20:13:55+01:00</updated><id>http://localhost:4001/feed.xml</id><title type="html">My portfolio</title><subtitle>Tommaso Vaccari&apos;s Portfolio</subtitle><entry><title type="html">Neural Networks : Linear Regression</title><link href="http://localhost:4001/blog/post/2025/01/25/NeuralNetworkP2.html" rel="alternate" type="text/html" title="Neural Networks : Linear Regression" /><published>2025-01-25T00:00:00+01:00</published><updated>2025-01-25T00:00:00+01:00</updated><id>http://localhost:4001/blog/post/2025/01/25/NeuralNetworkP2</id><content type="html" xml:base="http://localhost:4001/blog/post/2025/01/25/NeuralNetworkP2.html"><![CDATA[# Introduction

In this second post, we will take another step forward. We’ll dive into the concept of linear regression, explore its connection to neural networks, and demonstrate how to build a model from scratch. This approach will help us gain a deeper understanding of the core concepts behind both linear regression and neural networks.

## Linear Regression

### Model and Loss function

Before starting, we need to define some terminology. When we want to predict a value, we call it a label or target. Each label is associated with its own features. The model makes predictions based solely on these features. Therefore, we need to refine the model to accurately predict the correct label based on the provided input features.

We want to predict a  target, $ \hat{y}$, based on a set of features grouped in a vector called ${X}$. To make the prediction, we need to find the weight vector ${W}$ and the bias $ B $ that give us the most accurate predictions. This relationship can be expressed using the dot product:

<div class="mathjax-latex">
$$\hat{y} = W \cdot X + b $$
</div>

If we want to group everything together, we can define the X matrix, having along the lines the features of a single label, so we have the $\hat{y}$ vector that represents the vector of the prediction given the matrix features and the bias vector:

<div class="mathjax-latex">
$$\hat{y} = X \cdot W + b$$
</div>

Now we need to define a loss function to evaluate the model. The most common is the squared error, where $\hat{y}_i$ is the prediction and $y_i$ is the corresponding true label.

<div class="mathjax-latex">
$$L_i(W, b)= \frac{1}{2} (\hat{y}_i-y_i)^2$$
</div>

We can observe that the loss is a function of weight and bias.

Now we can extend the evaluation of the loss along all the predictions, averaging it to obtain an intuitive idea of how our model is performing.

<div class="mathjax-latex">
$$L_i (W,B) =\frac{1}{n} \sum_{i=1}^{n} L_i(W,B) = \frac{1}{n} \sum_{i=1}^{n} \frac{1}{2} (X^{(i)} \cdot W + b - y^{(i)})^2$$
</div>

We have to keep in mind our goal: we want to find W and B that give us the minimum value of loss along all the predictions.

### Minibatch Stochastic Gradient Descent

The key to optimizing a model and improving its predictions is to iteratively update the weights by modifying them in the opposite direction of the gradient of the loss function. This process is known as gradient descent. Although it may seem simple at first glance, it is the foundation of many advanced techniques in machine learning and plays a central role in training modern models.

To optimize the process, we do not update the model using the entire dataset at once. Instead, we randomly select smaller subsets of the data, called batches. The model is then optimized iteratively by performing updates on these batches, which makes the process more efficient and scalable, especially for large datasets. This approach is commonly referred to as mini-batch gradient descent.

To keep it simple we can break down this process into four steps:

1. Batch Selection:
   Randomly choose a small subset of training data. This approach balances computational efficiency with learning effectiveness. Instead of processing the entire dataset, which would be slow and memory-intensive, we sample a representative mini-batch that captures the overall data characteristics.
2. Loss Calculation:
   Measure how far the model's predictions are from the true values for each example in the batch. Compute the average loss, which serves as a performance metric. This average loss quantifies the model's current error, providing a clear signal about how well (or poorly) the model is performing on this particular set of examples.
3. Gradient Computation: Calculate the derivative of the loss with respect to each model parameter. This gradient acts like a compass, pointing to the direction that would most quickly increase the loss. By understanding how each weight contributes to the model's error, we can intelligently adjust the model's internal representation to improve its predictive capabilities.
4. Parameter Update: Move the model's parameters in the opposite direction of the gradient, scaled by a small learning rate. This is akin to taking careful steps down a complex landscape, where each step aims to reduce the overall error. The learning rate determines the size of these steps – too large, and you might overshoot the optimal solution; too small, and progress becomes painfully slow.

If we want to express this in formulas we have:

1) The weights update:

<div class="mathjax-latex">
$$
\mathbf{w} \gets \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \frac{\partial L^{(i)} (\mathbf{w}, b)}{\partial \mathbf{w}} = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \left( \mathbf{x}^{(i)} \cdot \mathbf{w} + b - y^{(i)} \right) \mathbf{x^{(i)}}.
$$
</div>

2) The Bias Update:

<div class="mathjax-latex">
$$
b \gets b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \frac{\partial L^{(i)} (\mathbf{w}, b)}{\partial b} 
= b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \left( \mathbf{x}^{(i)} \cdot \mathbf{w} + b - y^{(i)} \right).
$$
</div>

### Linear Regression as a Neural Network

While linear models are not sufficiently rich to express complex relationships between features, we can introduce neural networks to obtain a more expressive model. Nevertheless, we can also view a linear model as a neural network where every input feature corresponds to a neuron with its own weight and bias.

## Implementation using object-oriented design

To gain a deeper understanding of how a model is created and trained, we aim to implement the classes and methods from scratch. We are following this approach because, in my opinion, using machine learning libraries like PyTorch directly does not provide a comprehensive understanding of what happens under the hood.

```python
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import torch
```

Now we are ready to implement from scratch our model for linear regression, we need :

1. The model
2. The loss function
3. The optimization algorithm
4. The training function

### Building the model

```python
class LinearRegressionModel:
    def __init__(self, num_inputs, learning_rate, sigma=0.01):
            """
            Initialize the model parameters.

            Args:
            - num_inputs (int): Number of input features.
            - learning_rate (float): Learning rate for gradient descent.
            - sigma (float): Standard deviation for initializing weights.
            """
            self.num_inputs = num_inputs
            self.learning_rate = learning_rate
        
            # Initialize weights and bias
            self.w = torch.normal(mean=0.0, std=sigma, size=(num_inputs, 1), requires_grad=True)
            self.b = torch.zeros(1, requires_grad=True)
    
```

Here we have created a class that contains the weights and bias. Additionally, we have introduced the hyperparameters, which are typically user-defined and used to adjust various aspects during the training phase. The weights are sampled from a [normal distribution with a mean of 0 and a standard deviation of 0.01](https://en.wikipedia.org/wiki/Normal_distribution), while the bias is initialized to zero.

Now we can add the method to obtain the forward pass:

```python
@add_to_class(LinearRegressionModel) 
def forward(self, X):
        """
        Compute the forward pass: y = Xw + b.

        Args:
        - X (torch.Tensor): Input tensor of shape (batch_size, num_inputs).
    
        Returns:
        - torch.Tensor: Predicted values of shape (batch_size, 1).
        """
        return torch.matmul(X, self.w) + self.b
```

### Building the loss function

Now we add the method to calculate the loss for a single batch of examples by using the squared loss function and then returning the mean.

```python
@add_to_class(LinearRegressionModel)
def compute_loss(self, y_pred, y_true):
        """
        Compute Mean Squared Error loss.

        Args:
        - y_pred (torch.Tensor): Predicted values.
        - y_true (torch.Tensor): True values.
    
        Returns:
        - torch.Tensor: Scalar loss value.
        """
        return 0.5 * ((y_pred - y_true) ** 2).mean()
```

### Building the optimization algorithm

This is the fundamental part of our model—the algorithm that allows us to improve its predictions. We are going to implement Stochastic Gradient Descent (SGD), as discussed earlier.

The steps we want to follow are:

1. Randomly select a batch from the training set.
2. Make predictions using the selected batch.
3. Compute the loss of the predictions.
4. Calculate the gradient of the loss with respect to the weights and bias.
5. Update the parameters (weights and bias) using the learning rate and computed gradients.

```python
@add_to_class(LinearRegressionModel) 
def update_parameters(self):
        """
        Update the model parameters using gradient descent.
        """
        with torch.no_grad():
            self.w -= self.learning_rate * self.w.grad
            self.b -= self.learning_rate * self.b.grad

            # Manually zero the gradients
            self.w.grad.zero_()
            self.b.grad.zero_()

def train_step(self, X, y, batch_size):
    """
    Perform a single training step.

    Args:
    - X (torch.Tensor): Input data of shape (num_samples, num_inputs).
    - y (torch.Tensor): Target data of shape (num_samples, 1).
    - batch_size (int): Number of samples per batch.
  
    Returns:
    - float: Loss value for the batch.
    """
    # Sample a random batch
    num_samples = X.shape[0]
    indices = torch.randint(0, num_samples, (batch_size,))
    X_batch = X[indices]
    y_batch = y[indices]

    # Forward pass
    y_pred = self.forward(X_batch)

    # Compute loss
    loss = self.compute_loss(y_pred, y_batch)

    # Backward pass
    loss.backward()

    # Update parameters
    self.update_parameters()

    # Return the loss value as a scalar
    return loss.item()

```

### Building the training method

The last step to complete our model is to implement a method that allows us to train it. It would be useful to visualize how the weights, bias, and learning rate change throughout the training process, so we can also add a method to plot this information.

```python
@add_to_class(LinearRegressionModel)
def train(self, X, y, epochs, batch_size):
        """
        Train the model over multiple epochs.

        Args:
        - X (torch.Tensor): Input data of shape (num_samples, num_inputs).
        - y (torch.Tensor): Target data of shape (num_samples, 1).
        - epochs (int): Number of training epochs.
        - batch_size (int): Number of samples per batch.
    
        Returns:
        - list: List of loss values for each epoch.
        """
        losses = []

        for epoch in range(epochs):
            # Perform a training step and compute the average loss for the epoch
            loss = self.train_step(X, y, batch_size)
            losses.append(loss)

            # Print progress every 10 epochs
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.6f}")

        return losses
  
```

## Testing The model

Now that the class is built from scratch, we can proceed with testing it to verify if everything works correctly. Testing the model typically involves the following steps:

1. Creating synthetic data (or retrieving real data)
2. Instantiating the model
3. Training the model
4. Testing the model on the evaluation set

Now, we initialize a vector to represent the weights that our model needs to learn from the data during the training process, and we follow the same process for the bias.
Then we compute the target that we are going to use, with the features to train our model.
We can also adjust hyperparameters as needed.

```python
# Define hyperparameters
num_samples = 4000
num_inputs = 2000
learning_rate = 0.01
epochs = 2000
batch_size = 64

# Generate synthetic data
true_w = torch.randn((num_inputs, 1))
true_b = torch.rand((1,1))
X = torch.randn((num_samples, num_inputs))
y = torch.matmul(X, true_w) + true_b + torch.randn((num_samples, 1)) * 0.01
```

#### Define hyperparameters:

- **num_samples**: The number of data samples to generate.
- **num_inputs**: The number of input features for each sample.
- **learning_rate**: The rate at which the model learns during training.
- **epochs**: The number of times the entire dataset is passed through the model during training.
- **batch_size**: The number of samples processed before the model's internal parameters are updated.

#### Generate synthetic data:

- **true_w**: Randomly generated weights for the input features.
- **true_b**: Randomly generated bias term.
- **X**: Randomly generated input data with `num_samples` rows and `num_inputs` columns.
- **y**: The target values calculated by multiplying `X` with `true_w`, adding `true_b`, and adding a small amount of random noise to simulate real-world data.

Now we can look at the complete class with all the methods :

```python
class LinearRegressionModel:
    def __init__(self, num_inputs, learning_rate, sigma=0.01):
        """
        Initialize the model parameters.

        Args:
        - num_inputs (int): Number of input features.
        - learning_rate (float): Learning rate for gradient descent.
        - sigma (float): Standard deviation for initializing weights.
        """
        self.num_inputs = num_inputs
        self.learning_rate = learning_rate
    
        # Initialize weights and bias
        self.w = torch.normal(mean=0.0, std=sigma, size=(num_inputs, 1), requires_grad=True)
        self.b = torch.zeros(1, requires_grad=True)
        self.losses = []

    def forward(self, X):
        """
        Compute the forward pass: y = Xw + b.

        Args:
        - X (torch.Tensor): Input tensor of shape (batch_size, num_inputs).
    
        Returns:
        - torch.Tensor: Predicted values of shape (batch_size, 1).
        """
        return torch.matmul(X, self.w) + self.b

    def compute_loss(self, y_pred, y_true):
        """
        Compute Mean Squared Error loss.

        Args:
        - y_pred (torch.Tensor): Predicted values.
        - y_true (torch.Tensor): True values.
    
        Returns:
        - torch.Tensor: Scalar loss value.
        """
        return 0.5 * ((y_pred - y_true) ** 2).mean()

    def update_parameters(self):
        """
        Update the model parameters using gradient descent.
        """
        with torch.no_grad():
            self.w -= self.learning_rate * self.w.grad
            self.b -= self.learning_rate * self.b.grad

            # Manually zero the gradients
            self.w.grad.zero_()
            self.b.grad.zero_()

    def train_step(self, X, y, batch_size):
        """
        Perform a single training step.

        Args:
        - X (torch.Tensor): Input data of shape (num_samples, num_inputs).
        - y (torch.Tensor): Target data of shape (num_samples, 1).
        - batch_size (int): Number of samples per batch.
    
        Returns:
        - float: Loss value for the batch.
        """
        # Sample a random batch
        num_samples = X.shape[0]
        indices = torch.randint(0, num_samples, (batch_size,))
        X_batch = X[indices]
        y_batch = y[indices]

        # Forward pass
        y_pred = self.forward(X_batch)

        # Compute loss
        loss = self.compute_loss(y_pred, y_batch)

        # Backward pass
        loss.backward()

        # Update parameters
        self.update_parameters()

        # Return the loss value as a scalar
        return loss.item()

    def train(self, X, y, epochs, batch_size):
        """
        Train the model over multiple epochs.

        Args:
        - X (torch.Tensor): Input data of shape (num_samples, num_inputs).
        - y (torch.Tensor): Target data of shape (num_samples, 1).
        - epochs (int): Number of training epochs.
        - batch_size (int): Number of samples per batch.
    
        Returns:
        - list: List of loss values for each epoch.
        """
    

        for epoch in range(epochs):
            # Perform a training step and compute the average loss for the epoch
            loss = self.train_step(X, y, batch_size)
            self.losses.append(loss)

            #print(loss)
    
        print("Final Loss : ",loss)
        self.plot_training_results()

        #return losses


    def plot_training_results(self):
        plt.figure(figsize=(25, 8))
    
        # First subplot: Training loss 
        plt.subplot(1, 3, 1)
        plt.plot(np.log(self.losses), label="Log Loss", color="blue")
        plt.axhline(np.log(self.losses[-1]), linestyle="--", color="red", label="Final Loss")
        plt.xlabel('Epoch')
        plt.ylabel('Log Loss')
        plt.title('Training Loss Over Time')
        plt.legend()
        plt.grid(True)
    
        # Second subplot: weight comparison
        plt.subplot(1, 3, 2)
        learned_weights = self.w.detach().numpy().flatten()
        true_weights = true_w.numpy().flatten()
    
        # Create a scatter plot comparing true vs learned weights
        plt.scatter(true_weights, learned_weights, alpha=0.5, color='blue')
    
        # Add a diagonal line representing perfect prediction
        max_val = max(np.max(true_weights), np.max(learned_weights))
        min_val = min(np.min(true_weights), np.min(learned_weights))
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Perfect Match')
    
    
        plt.xlabel('True Weights')
        plt.ylabel('Learned Weights')
        plt.title('True vs Learned Weights')
        plt.legend()
        plt.grid(True)
    
        # Third subplot: Bias comparison 
        plt.subplot(1, 3, 3)
        true_bias = float(true_b.numpy().flatten()[0])
        learned_bias = float(self.b.detach().numpy().flatten()[0])
    
        x = ['True Bias', 'Learned Bias']
        y = [true_bias, learned_bias]
        plt.bar(x, y, color=['blue', 'orange'], alpha=0.7)
    
        for i, val in enumerate(y):
            plt.text(i, val + 0.02, f'{val:.2f}', ha='center', fontsize=10)
        
        plt.ylabel('Bias Value')
        plt.title('True vs Learned Bias')
        plt.grid(axis='y')
    
        plt.tight_layout()
        plt.show()
```

```python
Model = LinearRegressionModel(num_inputs,learning_rate)
Model.train(X,y,epochs,batch_size)
```

    Final Loss :  0.07868940383195877

![png](/assets/images/output_21_1.png)

## Conclusion

This example, while simple, highlights some key concepts: we can view a linear regression model as a neural network, and we built this model from scratch without relying on the high-level APIs provided by PyTorch. In this second part of our machine learning series, we’ve taken a step toward higher-level abstraction. In the first post, we explored how each component of a neural network is built under the hood. Here, we leveraged PyTorch’s tensor implementation but still created the model from scratch with everything we needed to train it on our data.]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Building a Simple Web App with Firebase Authentication and Firestore: A Step-by-Step Guide</title><link href="http://localhost:4001/blog/post/2024/08/28/Eisenhower_Matrix.html" rel="alternate" type="text/html" title="Building a Simple Web App with Firebase Authentication and Firestore: A Step-by-Step Guide" /><published>2024-08-28T00:00:00+02:00</published><updated>2024-08-28T00:00:00+02:00</updated><id>http://localhost:4001/blog/post/2024/08/28/Eisenhower_Matrix</id><content type="html" xml:base="http://localhost:4001/blog/post/2024/08/28/Eisenhower_Matrix.html"><![CDATA[In today’s post, I’ll walk you through the development of a simple web application, showcasing how to leverage Firebase Authentication and Firestore Database. This guide will help you understand how to:

1. Authenticate users with Firebase Authentication.
2. Store and retrieve data in real time using Firestore.

To make things more practical, we’ll build an application based on the [Eisenhower Matrix](https://en.wikipedia.org/wiki/Time_management#The_Eisenhower_Method), a powerful decision-making framework that helps you prioritize tasks based on urgency and importance.

If you want to see the final result in action, feel free to explore the app [here](https://eisenhower-matrix.tommasovaccari.com).

## What is the Eisenhower Matrix?

The Eisenhower Matrix is divided into four quadrants:

- **Quadrant 1**: Urgent and Important
- **Quadrant 2**: Not Urgent but Important
- **Quadrant 3**: Urgent but Not Important
- **Quadrant 4**: Neither Urgent nor Important

This framework helps you classify tasks and make better decisions about what to prioritize, what to plan for later, and what to delegate or remove.

## Overview of the App

My web application brings the Eisenhower Matrix to life allowing users to:

- **Create an account** using Firebase Authentication.
- **Add tasks** and assign them to one of the four quadrants (Urgent/Important, Not Urgent/Important, Urgent/Not Important, Not Urgent/Not Important).
- **Move tasks dynamically** between quadrants as priorities change.
- **Store tasks** securely in the cloud using Firestore, ensuring persistence across sessions and accessibility from any device.

### Core Components

- **Firebase Authentication**: Enables users to create accounts, log in, and secure access to their personalized task data.
- **Firestore Database**: A NoSQL cloud database that stores user tasks in real time, ensuring that changes are immediately reflected in the UI and synced across devices.
- **HTML, JavaScript, and CSS**: The front-end is built using standard web technologies. HTML structures the layout, JavaScript provides interactivity (e.g., adding and moving tasks), and CSS ensures the app is visually appealing and user-friendly.

## Why Firebase?

Firebase provides a robust suite of tools, making it ideal for small projects like this. It offers both authentication and database features, allowing you to manage user sessions and store data without having to set up complex backend infrastructure. The **free tier** is more than enough for small projects, allowing you to build and launch an app without any initial cost.

## Step-by-Step Guide

Now, let's dive into the app construction, step by step.

### Step 1: Setting Up Firebase

1. Go to the [Firebase Console](https://console.firebase.google.com) and create a new project.
2. Enable **Firebase Authentication**: Navigate to the "Authentication" tab, click "Set up sign-in method", and enable **Email/Password** authentication.
3. Enable **Firestore Database**: In the "Firestore Database" section, click "Create database", and choose the appropriate settings for your app (test mode is fine for development).

Now you're ready to integrate Firebase into your app!

### Step 2 : Integrate Firebase into the Web App

Once you've completed the basic structure and styling of your web application using HTML and CSS, you can now integrate Firebase services to handle authentication and data storage.

To do this, you need to import the necessary Firebase modules directly into your JavaScript file. Firebase provides a modern way to handle individual services using ES modules, which makes your code more modular and lightweight.

```javascript
import { initializeApp } from "https://www.gstatic.com/firebasejs/9.0.0/firebase-app.js";
import { getAuth, signInWithEmailAndPassword, createUserWithEmailAndPassword, onAuthStateChanged, signOut } from "https://www.gstatic.com/firebasejs/9.0.0/firebase-auth.js";
import { getFirestore, doc, setDoc, collection, addDoc, onSnapshot, deleteDoc, query, where, limit, startAfter } from "https://www.gstatic.com/firebasejs/9.0.0/firebase-firestore.js";

```

- initializeApp: This function is essential to start using Firebase in your app. It initializes Firebase using your project’s configuration details.

Authentication Services:

- getAuth: Initializes Firebase Authentication, which is used to manage users and their login status..
- signInWithEmailAndPassword: Allows users to log in using their email and password.
- createUserWithEmailAndPassword: Enables new user registration with email and password.
- onAuthStateChanged: Listens to changes in authentication state, allowing you to track whether a user is logged in or logged out.
- signOut: Provides a method to log users out of the application.

Firestore Services:

- getFirestore: Initializes Firestore, the cloud database service that stores and syncs data in real time.
doc and setDoc: Used to create or reference specific documents in the Firestore database (for example, tasks in our Eisenhower Matrix app).
- collection and addDoc: Create or add data to a Firestore collection, such as adding a task to a user's list.
- onSnapshot: Listens to real-time changes in the database, so your app automatically reflects any updates made by the user.
- deleteDoc: Allows you to delete specific documents (e.g., remove tasks).
- query, where, limit, startAfter: Firestore query methods, which let you filter, paginate, and retrieve data efficiently from large collections.

By importing these modules, you only load the specific functionality you need for your web app, which is a more efficient approach, especially for smaller projects.

### Step 3 : Firebase Configuration and Initialization

Now you have to obtain the setting of your firebase project. You can find them in the setting section on the firebase console.

In your javascript file, initialize Firebase using the config setting :

```javascript
// Firebase config (replace with your own Firebase project settings)
const firebaseConfig = {
    apiKey: "YOUR_API_KEY",
    authDomain: "YOUR_AUTH_DOMAIN",
    projectId: "YOUR_PROJECT_ID",
    storageBucket: "YOUR_STORAGE_BUCKET",
    messagingSenderId: "YOUR_MESSAGING_SENDER_ID",
    appId: "YOUR_APP_ID"
};

const app = initializeApp(firebaseConfig);
const auth = getAuth(app);
const db = getFirestore(app);

```

### Step 4: Implement Firebase Authentication

Now that Firebase is initialized, let’s handle user authentication. We’ll add functionality for both signing up and logging in.
Now for the sake of clarity I use as an ID 'signUp' and 'logn' ,then you have to change it with the ID that you used in your HTML.

```javascript
// Sign up new users
document.getElementById('signUp').addEventListener('click', () => {
    const email = document.getElementById('email').value;
    const password = document.getElementById('password').value;
    
    auth.createUserWithEmailAndPassword(email, password)
        .then(userCredential => {
            alert("User signed up!");
        })
        .catch(error => {
            alert(error.message);
        });
});

// Log in existing users
document.getElementById('login').addEventListener('click', () => {
    const email = document.getElementById('email').value;
    const password = document.getElementById('password').value;
    
    auth.signInWithEmailAndPassword(email, password)
        .then(userCredential => {
            alert("User logged in!");
        })
        .catch(error => {
            alert(error.message);
        });
});

```

### Step 5: Firestore for Storing Tasks

Once the user is logged in, we’ll allow them to add and remove tasks to Firestore. Each task will be saved under the user’s unique ID in the Firestore database.

```javascript
// Add a task to Firestore
function addTask(quadrantId, task) {
    const userId = getCurrentUserId();
    if (!userId) return;

    const tasksRef = collection(db, `users/${userId}/tasks`);
    addDoc(tasksRef, {
        text: task,
        quadrant: quadrantId,
        createdAt: new Date()
    }).then(() => {
        //console.log("Task added successfully");
    }).catch((error) => {
        console.error("Error adding task: ", error);
    });
}
//Remove task from firestore
function deleteTask(taskId, taskItem) {
    const userId = getCurrentUserId();
    if (!userId) return;

    const taskRef = doc(db, `users/${userId}/tasks/${taskId}`);
    deleteDoc(taskRef).then(() => {
        taskItem.parentNode.removeChild(taskItem);
        console.log("Task deleted successfully");
    }).catch((error) => {
        console.error("Error deleting task: ", error);
    });
}

```

### Step 6: Displaying Tasks in Real Time

Firestore provides real-time listeners, which we’ll use to display tasks as soon as they’re added.

```javascript
auth.onAuthStateChanged(user => {
    if (user) {
        // Listen for changes to the user's task list in Firestore
        db.collection('users').doc(user.uid).collection('tasks')
            .orderBy('timestamp')
            .onSnapshot(snapshot => {
                const taskList = document.getElementById('taskList');
                taskList.innerHTML = '';  // Clear previous tasks
                
                snapshot.forEach(doc => {
                    const taskItem = document.createElement('li');
                    taskItem.textContent = doc.data().task;
                    taskList.appendChild(taskItem);
                });
            });
    }
});

```

### Step 7: Putting It All Together

At this point, we’ve successfully integrated Firebase Authentication and Firestore. Here’s what happens in our app:

1. User Authentication: Users can sign up or log in.
2. Task Management: Once authenticated, users can add tasks to Firestore.
3. Real-Time Updates: Tasks are displayed in real-time, so users can see updates immediately.

## Conclusion

Integrating Firebase Authentication and Firestore into your web application offers a streamlined, serverless solution for managing both user authentication and real-time data storage. In our example, we demonstrated how to build a task management app using the Eisenhower Matrix, but the principles outlined here can easily be extended to a wide range of web applications. Firebase’s suite of tools enables you to focus on your front-end development while it handles the complexities of authentication, database management, and scalability.

What makes Firebase especially appealing is its simplicity and scalability—whether you're working on small projects or building apps with large-scale needs, Firebase can handle it. The free tier is particularly beneficial for smaller applications or prototypes, allowing you to develop and deploy without upfront costs.

If you're looking to expand beyond this, Firebase offers a rich ecosystem of services, including Cloud Functions, Hosting, and Analytics, which can further enhance your app's functionality and performance. These tools, combined with Firebase’s real-time synchronization and seamless cross-platform support, can significantly reduce development time and effort, while ensuring a robust and scalable application.

Explore Firebase’s extensive [documentation](https://firebase.google.com/docs/) to dive deeper into these tools, and take your web app to the next level. Happy coding!]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In today’s post, I’ll walk you through the development of a simple web application, showcasing how to leverage Firebase Authentication and Firestore Database. This guide will help you understand how to:]]></summary></entry><entry><title type="html">Building a Daily Vocabulary Newsletter with Python, Google Docs, and Google Sheets</title><link href="http://localhost:4001/blog/post/2024/08/21/EnglishNewsletter.html" rel="alternate" type="text/html" title="Building a Daily Vocabulary Newsletter with Python, Google Docs, and Google Sheets" /><published>2024-08-21T00:00:00+02:00</published><updated>2024-08-21T00:00:00+02:00</updated><id>http://localhost:4001/blog/post/2024/08/21/EnglishNewsletter</id><content type="html" xml:base="http://localhost:4001/blog/post/2024/08/21/EnglishNewsletter.html"><![CDATA[In today’s post, I’d like to share a simple app I developed to improve my English vocabulary. This app is a daily newsletter that sends me and to other subscribers  vocabulary words, and the twist is that it uses Google Docs as the database for storing the vocabulary words and their meanings, while Google Sheets is used to manage user subscriptions and track the words I've already studied.

I decided to create this tool because I’m actively learning English and wanted an automated way to refresh and expand my vocabulary. Since I have experience with Python and APIs, I took the opportunity to integrate Google Docs and Google Sheets APIs to store and manage vocabulary words and user data.

You can see the code in my [github repo](https://github.com/T-vaccari/VocabularyNewsletter)

## Overview of the app

Core Components:

1. Google Docs: Serves as the vocabulary database, containing a list of words and their meanings.
2. Google Sheets: Manages subscriber information and tracks vocabulary word appearances.
3. Email Automation: Sends out daily emails with a curated list of vocabulary words.

Here’s how everything fits together:

- Google Docs stores the vocabulary words along with their meanings, formatted like this:
word ||| meaning.
- Google Sheets maintains a list of recipients, along with metadata such as which document they are pulling their words from and whether they should receive an email on any given day.
- The app pulls a random set of words from Google Docs, formats them into an email, and sends it to the user.

## Why using Google Suit and relative APIs

There are many reasons I chose Google Docs and Google Sheets for this app:

- Google Docs: It allows me to easily add, update, and manage vocabulary terms in a simple, familiar text environment.
- Google Sheets: This is a natural choice for tracking user data, such as which words have already been sent, email preferences, and other metadata.
- Google API Integration: Google provides powerful APIs that allow seamless interaction with both Docs and Sheets, making it easy to read, write, and update data programmatically.

## How It Works

### 1. Fetching Recipients and Their Vocabulary Database from Google Sheets

The Google Sheet must be structured as follows: Email, DOCUMENT ID, SHEET ID, and a flag to receive the newsletter.

![Sheet](/assets/images/IMG_123.png)

The function `start_vocab_app()` retrieves recipient details, including their vocabulary document and tracking sheet IDs. It then uses the provided Google Sheets API to extract this data and determine which subscribers should receive the newsletter.

```python
def start_vocab_app():
  SHEET_ID = ""  # Insert the SHEET ID used for the database
  SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
  credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
  service = build('sheets', 'v4', credentials=credentials)
  SHEET_RANGE = ""  # Range to read data in the SHEET
  sheet = service.spreadsheets()
  result = sheet.values().get(spreadsheetId=SHEET_ID, range=SHEET_RANGE).execute()
  values = result.get('values', [])
  recipients = dict()
  for item in values[1:]:
    recipients[item[0]] = [thing for thing in item[1:]]

  for email, features in recipients.items():
    try:
      if (features[-1]).lower() == "no":
        words = read_google_doc(features[0])
        email_body = create_email_body(words)
        send_email(email, email_body)
      
        if features[1] != '.':
          counting_words(words, features[1])

        else:
            continue
      
    except Exception as e:
      logging.error(f"Errore per il destinatario {email}: {e}")
      print(f"Errore con {email}. Passo al destinatario successivo.")
      continue
```

### 2. Fetching Vocabulary from Google Docs

The app reads vocabulary words from a specific Google Doc using the Google Docs API. Each line in the document is formatted as a word and its meaning separated by \|\|\| (I used it for parsing pursuits).Here is how it mus look like to work with the script :

![DOC](/assets/images/IMG_124.png)

Here’s the part of the code that reads the words and returns the one that are selected randomly :

```python
def read_google_doc(DOCUMENT_ID):
    SCOPES = ['https://www.googleapis.com/auth/documents.readonly']
    wordstosend = 
    credentials = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    service = build('docs', 'v1', credentials=credentials)

    doc = service.documents().get(documentId=DOCUMENT_ID).execute()
    content = doc.get('body').get('content')

    text = ""
    for element in content:
        if 'paragraph' in element:
            elements = element.get('paragraph').get('elements')
            for elem in elements:
                text_run = elem.get('textRun')
                if text_run:
                    text += text_run.get('content')

    lines = text.strip().split('\n')
    term_list = []
    
    for line in lines:
        if '|||' in line:
            term, meaning = line.split('|||')
            term_list.append([term.strip(), meaning.strip()])

    number = min(len(term_list), wordstosend)
    random.shuffle(term_list)
    return random.sample(term_list, number)


```

### 3. Tracking Logs in Google Sheet

We maintain a log of each word that is sent through the newsletter to ensure accurate tracking and to see what words the subscribers has already learned. Here's the code :

``` python
def counting_words(words, SHEET_ID):
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']
    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    service = build('sheets', 'v4', credentials=credentials)
    SHEET_RANGE = ""
    sheet = service.spreadsheets()
    result = sheet.values().get(spreadsheetId=SHEET_ID, range=SHEET_RANGE).execute()
    values = result.get('values', [])
    my_dict = {row[0]: row for row in values[1:]}  

    modified_values = [values[0]]  
    english_terms = list()

    for terms in words:
        english_terms.append(terms[0])
    
    for row in values[1:]:
        if row[0] in english_terms:
            row[1] = str(int(row[1]) + 1)
        modified_values.append(row)

    for word in english_terms:
        if word not in my_dict:
            modified_values.append([word, '1'])

    body = {'values': modified_values}

    result = service.spreadsheets().values().update(
        spreadsheetId=SHEET_ID,
        range=SHEET_RANGE,
        valueInputOption="RAW",
        body=body
    ).execute()


```

### 4. Creating the body of the email

This function generates an HTML email with vocabulary words and their meanings. It formats each word-meaning pair into a styled, centered HTML layout using basic CSS for a clean appearance. The email includes a header, a list of words for the day. The function returns the email content as an HTML string, ready for sending.

```python
def create_email_body(words):
    word_html = "".join(
        f"<div style='text-align: center; margin: 10px 0; font-size: 24px;'><strong>{word_pair[0]} : {word_pair[1]}</strong></div>"
        for word_pair in words
    )

    return f"""
    <html>
    <head>
      <style>
        body {{
          font-family: Arial, sans-serif;
          background-color: #f9f9f9;
          padding: 20px;
        }}
        h1 {{
          color: #4A90E2;
          text-align: center;
          margin-bottom: 10px;
        }}
        h2 {{
          color: #333;
          text-align: center;
          margin-bottom: 20px;
        }}
        p {{
          font-size: 16px;
          line-height: 1.5;
        }}
        .container {{
          max-width: 600px;
          margin: 0 auto;
          background-color: #fff;
          padding: 30px;
          border-radius: 10px;
          box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }}
        .footer {{
          text-align: center;
          margin-top: 20px;
          font-size: 14px;
          color: #888;
        }}
      </style>
    </head>
    <body>
      <div class="container">
        <h1>Daily Vocabulary</h1>
        <h2>Here are the words for today!</h2>
        {word_html}
        <div class="footer">
          <p>Keep Learning!</p>
          <p style='font-style: italic;'>Mail sent automatically. Please do not respond.</p>
        </div>
      </div>
    </body>
    </html>
    """

```

### 5. Sending the Email

Once the words are fetched and the progress tracked, the app creates an HTML email with the vocabulary words and sends it to the recipient using SMTP:

```python
def send_email(EMAIL_DESTINATION, email_body):
    msg = MIMEMultipart()
    msg['Subject'] = 'Daily Vocabulary'
    msg['From'] = EMAIL_ADDRESS
    msg['To'] = EMAIL_DESTINATION
    msg.attach(MIMEText(email_body, 'html'))

    try:
        with smtplib.SMTP_SSL('smtp.gmail.com', 465) as smtp:
            smtp.login(EMAIL_ADDRESS, EMAIL_PASSWORD)
            smtp.send_message(msg)
        print(f'Email inviata a {EMAIL_DESTINATION}')
    except Exception as e:
        
        logging.error(f"Errore nell'invio dell'email a {EMAIL_DESTINATION}: {e}")
        print(f"Errore nell'invio dell'email a {EMAIL_DESTINATION}: {e}")

```

## Conclusion

Developing this daily vocabulary newsletter has been both a fulfilling and insightful project, blending the power of Python with the versatility of Google APIs to automate and enrich the language-learning experience. By seamlessly integrating Google Docs and Google Sheets, the application efficiently handles vocabulary data, user subscriptions, and content distribution, making the learning process smoother and more interactive.

Beyond enhancing my English vocabulary, this tool showcases the practical benefits of leveraging APIs to streamline workflows and automate repetitive tasks. Whether you're exploring new ways to master a language or seeking methods to boost productivity, this project underscores the potential of technology to create tailored and efficient solutions.

If you found value in this article, I encourage you to explore similar integrations and push the boundaries of API-driven applications. Don’t hesitate to reach out if you have any questions or want to share your own experiences with building automated tools — the possibilities are endless, and innovation is just a step away.]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[In today’s post, I’d like to share a simple app I developed to improve my English vocabulary. This app is a daily newsletter that sends me and to other subscribers vocabulary words, and the twist is that it uses Google Docs as the database for storing the vocabulary words and their meanings, while Google Sheets is used to manage user subscriptions and track the words I’ve already studied.]]></summary></entry><entry><title type="html">Neural Networks : Introduction</title><link href="http://localhost:4001/blog/post/2024/07/20/NeuralNetworkP1.html" rel="alternate" type="text/html" title="Neural Networks : Introduction" /><published>2024-07-20T00:00:00+02:00</published><updated>2024-07-20T00:00:00+02:00</updated><id>http://localhost:4001/blog/post/2024/07/20/NeuralNetworkP1</id><content type="html" xml:base="http://localhost:4001/blog/post/2024/07/20/NeuralNetworkP1.html"><![CDATA[## Introduction

In this post, I would like to introduce how neural networks are built under the hood. I will explore the essential components and mechanisms that enable neural networks to learn and make predictions. By the end of this article, you should have a clearer understanding of how these powerful models work under the hood.

## Overview of Neural Networks

Before diving deep into neural networks, I would like to roughly remind the reader of the main goal and how it is reached. Neural networks (NNs) are a class of machine learning models inspired by the structure and function of the human brain. The primary objective of a neural network is to learn a mapping from input data to desired output values by adjusting its parameters through a process of optimization.

### 1. Goal of neural networks

The central aim of a neural network is to learn how to produce accurate outputs when given specific inputs. This process involves training the network on a dataset, where each data point consists of an input and a corresponding target output. For instance, in a classification problem, the input might be an image, and the target output would be the label of the object in the image. Or maybe in a regression problem, we want to train our model to answer the question, how much?

### 2. Training Process

#### 2.1 Feeding data into the net and comparison of the output

During training, the network is provided with a set of input data and its corresponding target outputs. The network makes predictions based on the input data, which are then compared to the target outputs. This comparison is crucial for evaluating how well the network is performing.

#### 2.2 Loss Function

To quantify the difference between the network's predictions and the target outputs, we use a loss function (or cost function). The loss function measures the prediction error. Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy Loss for classification tasks. A lower value of the loss function indicates better performance of the network.

#### 2.3 Optimization via backpropagation

The goal of training is to minimize the loss function, which involves finding the optimal set of parameters (weights and biases) for the network. To achieve this, we use an optimization algorithm that adjusts the parameters to reduce the loss. Backpropagation is the method used to compute the gradients of the loss function with respect to the network's parameters. It involves:

- Forward Pass: Computing the predictions of the network and the loss.
- Backward Pass: Calculating the gradients of the loss function with respect to each parameter using the chain rule of calculus.

#### 2.4 Gradient Descent

Once the gradients are computed, they are used by the optimization algorithm (often gradient descent or its variants) to update the network's parameters. The network parameters are adjusted in the direction that reduces the loss function. This process is iterated over multiple epochs (passes through the entire dataset) until the loss function converges to a minimum value or sufficiently small error.

Roughly speaking, this is the process that we want to follow to achieve our goal. With the foundational concepts established, we will now delve into the detailed steps required to build a neural network. Firstly, we need to create a class that represents data within our neural network. This class must track the origin of each data value, including the operations and values used to compute it, in order to facilitate accurate gradient calculation during backpropagation. We are going to call the class for representing data *Value* class.

## Breakdown of the code for the Value class

Now I will start to implement in code what we have seen in the foundational concepts about NN.

### Building the basic item of the class and the attribute of the object

```python
class Value: 
  
    def __init__(self, data, _children=(), _op=''):
    
        self.data = data
        self._previously = set(_children)
        self._operations = _op
        self.grad = 0.0
        self._backward = lambda: None

    def __repr__(self):
        return f"Value(data={self.data})"
```

Here we initialize the value objects with its own attribute. Let's break down each attribute:

- self.data = data, saves the value of the object
- self._previously = set(_children), saves in a set the children of its value, that means that we save what values generated this value. Thanks to the set we can avoid duplicates
- self._operations = _op, we keep track of what operation generated this value
- self.grad = 0.0, we initially set it to zero, then we are going to modify it accordingly to the rules of backpropagation, we store it in this attribute
- self._backward = lambda: None, we need to store here the function that provides us the gradient, we set it initially to a lambda None because it depends on the operation that generated this Value object.

The __repr__ method is used to print the value object. So for example, if we want to generate a value object we can do as follows:

```python
a = Value(3)
print(a)
>>> Value(data=3)
```

### Building the basic operation and the relative backward function for value object

```python
class Value: 
    ... # existing code

    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), '+')
  
        def _backward():
            self.grad += 1.0 * out.grad
            other.grad += 1.0 * out.grad
        out._backward = _backward
  
        return out
```

Here we have the code for the __add__ method. Simply in this method, we create an out Value object featured by a new value and a set of children (the two Value objects that originated it) and then we append the operation that originated that new Value object. Then from the fundamentals of calculus, we know that the gradient of the two children due to this operation can be calculated as shown in the code. For much more detail watch gradient explained [here](https://en.wikipedia.org/wiki/Gradient). Let's watch how the add function behaves:

```python
a = Value(3)
b = Value(4)
c = a + b
print(c)
>>> Value(data=7)
print(c._previously)
>>> (Value(data=3), Value(data=4))
print(c._operations)
>>> '+'
```

Now let's look at the code for the __mul__ method:

```python
class Value:
    ... # existing code

    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), '*')
    
        def _backward():
            self.grad += other.data * out.grad
            other.grad += self.data * out.grad
        out._backward = _backward
    
        return out
```

In the __add__ method, we create a new Value object with a new value and a set of children, which are the two Value objects that originated this result. We also append the operation that produced this new Value object. The backward function is designed to compute the gradient according to calculus rules.

Note that we use the += operator when updating the .grad attribute. This is because we want to accumulate the gradient. For example, if the Value object is involved in multiple operations, the gradient should reflect all these operations, making the gradient cumulative. Generally, this is the process that we follow to create new methods for operations for this class. For the purpose of creating a neural network, we need also a function that can work for us as an [activation function](https://en.wikipedia.org/wiki/Activation_function). In this case, we add the tanh function for this scope.

```python
class Value:
    ... # existing code

    def tanh(self):
        x = self.data
        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)

        out = Value(t, (self,), 'tanh')
  
        def _backward():
            self.grad += (1 - t**2) * out.grad
        out._backward = _backward
    
        return out
```

From calculus, we know that tanh is defined as:

$$
\tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
$$

Then we can implement the backward function knowing that:

$$
\frac{d}{dx} \tanh(x) = 1 - \tanh^2(x)
$$

Now we implement the exponential method for being able to use tanh:

```python
class Value:
    ... # existing code

    def exp(self):
        x = self.data
        out = Value(math.exp(x), (self,), 'exp')
    
        def _backward():
            self.grad += out.data * out.grad 
        out._backward = _backward
    
        return out
```

We have now implemented most of the basic operations for our Value object. The final step is to implement a backward method. This method will be responsible for calling the backward functions of each object in the proper order.

## Building the backward method

Here's the code for the function:

```python
class Value: 
    ... # existing code

    def backward(self):
  
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._previously:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
    
        self.grad = 1.0
        for node in reversed(topo):
            node._backward()
```

Thanks to the implementation of the Value object, we are able to construct a computation graph that captures the entire history of how a Value object was derived. This graph is formed by following the _previously set of each Value object, which tracks all the predecessor values involved in its computation.

To compute the gradients, we begin by focusing on the final Value object, which typically results from a loss function in a neural network. Since we are interested in the gradient of this final Value with respect to itself, we initialize its gradient (self.grad) to 1. This initialization signifies that the gradient of the final Value with respect to itself is 1.

Following this, we execute the backward pass by invoking the _backward function for each Value object. This process starts with the final Value and proceeds backward through the computation graph to the initial Value objects. This reverse traversal ensures that the gradient for each Value is computed correctly based on the chain rule of differentiation, allowing us to accumulate the gradients appropriately.

We've now looked at the basics of how a neural network works. This simple approach helps us understand the core concepts behind its operation. Now it's time to create a very simple net.

### Creating a Neural Network

Now we will look more deeply into how to create a net by little steps.

#### Create one artificial neuron

Now we can create an [artificial neuron](https://en.wikipedia.org/wiki/Artificial_neuron) with two inputs. This is an example of how a neuron works:

```python
# Inputs x1, x2
x1 = Value(1.0)
x2 = Value(2.0)
# Weights w1, w2
w1 = Value(6.0)
w2 = Value(-4.0)
# Bias of the neuron
b = Value(9.8)
# x1*w1 + x2*w2 + b
x1w1 = x1 * w1
x2w2 = x2 * w2
x1w1x2w2 = x1w1 + x2w2
n = x1w1x2w2 + b
# Activation function
o = n.tanh()
```

o is the output of our neuron. The next step is to define the class Neuron, here's the code:

```python
class Neuron:
  
    def __init__(self, nin):
        self.w = list()
        self.b = Value(random.uniform(-1, 1))

        for _ in range(nin):
            self.w.append(Value(random.uniform(-1, 1)))
  
    def __call__(self, x):
        # w * x + b
        activation = 0
        for wi, xi in zip(self.w, x):
            activation += wi * xi
        activation += self.b
        out = activation.tanh()

        return out
  
    def parameters(self): # For storing the parameters
        return self.w + [self.b]
```

Here we define the class Neuron. In the __init__ method, we create the weights based on the number of inputs (nin) and then we create the bias Value. When we call the Neuron, it performs the activation based on the input and gives as output the result. Here's an example of usage:

```python
x = [1, 2, 3]
a = Neuron(3)
print(a(x))
>>> Value(data=something)
```

#### Create a layer

The next step is to create a layer made of neurons, here's the code:

```python
class Layer:
  
    def __init__(self, nin, nout):
        self.neurons = list()

        for _ in range(nout):
            self.neurons.append(Neuron(nin))
  
    def __call__(self, x):
        outs = list()
    
        for neuron in self.neurons:
            outs.append(neuron(x))

        return outs[0] if len(outs) == 1 else outs
  
    def parameters(self):
        return [p for neuron in self.neurons for p in neuron.parameters()]
```

In the __init__ method, we initialize a layer specifying the number of inputs for each neuron (nin) and the number of outputs for the layer (nout). When we call it, the __call__ method pushes the input to every neuron and returns the output of every neuron.

#### Create the multilayer perceptron

The final step is to create an [MLP fully connected](https://en.wikipedia.org/wiki/Multilayer_perceptron), here's the code:

```python
class MLP:
  
    def __init__(self, nin, nouts):
        sz = [nin] + nouts
        self.layers = list()

        for i in range(len(nouts)):
            self.layers.append(Layer(sz[i], sz[i+1]))
  
    def __call__(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
  
    def parameters(self):
        return [p for layer in self.layers for p in layer.parameters()]
```

In the __init__ method, as usual, we initialize the MLP by giving the number of inputs of the neurons (nin) and then a list (nouts) where we store the number of neurons for each layer. When we create the self.layer attribute, we iterate over a list that we create where we have the number of inputs and outputs required for every layer. Now let's look at how we can use it:

```python
x = [1.0, 5.0, -6.0]
net = MLP(3, [4, 4, 1])
net(x)
>>> Value(data=something)
```

Now we are ready to train the net that we have built.

#### Creating the dataset

To start, we can manually create a very easy dataset and the desired target:

```python
xs = [
    [1.0, 2.0, -1.0],
    [6.0, -4.0, 1],
    [2, 1.0, -1.0],
    [4.0, 3.0, -2.0],
]
ys = [2.0, -4.0, -3.0, 2.0]
```

So when we feed our net the first list, we want to obtain as a result the first element of the ys list. Now all we need is a loss function that can tell us how good the output of our net is. When we have our loss function, we can call the backpropagation on it (remember that it is a Value object so we can do it) and then we can slightly adjust the parameters according to what minimizes the loss function. Now we can implement it:

```python
for k in range(20):
  
    # forward pass
    ypred = list()
    for x in xs:
        ypred.append(net(x))
    loss = 0

    for ygt, yout in zip(ys, ypred):
        loss += (yout - ygt)**2

    # backward pass
    for p in net.parameters():
        p.grad = 0.0
    loss.backward()

    # update
    for p in net.parameters():
        p.data += -0.1 * p.grad

    print(k, loss.data)
```

In this piece of code, we train our net following these steps:

1. Forward pass
   We feed the net the dataset and then we save the output of the net. Then we evaluate the loss using the [MSE function](https://en.wikipedia.org/wiki/Mean_squared_error).
2. Backward Pass
   We reset all the gradients of all parameters and then we compute the new gradients of each parameter by calling backward on the loss function.
3. Update
   We update the parameters in the direction that minimizes the loss function using a learning rate of 0.1.

## Conclusion

In this first part, we built the fundamentals for understanding neural networks. We built our own api to implement it to have a deeper understand of how they work.
In the next part, we are going to see how to implement a neural network for linear regression using an higher level approach.]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>